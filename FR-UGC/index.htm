<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>BVI-UGC</title>
 <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/5.0.0/normalize.min.css">
       <link rel="stylesheet" href="../css/style.css">
</head>

<body id="page-top">
    <div class="page-container">
        <div class="inner">
            <div class="container">
                <div class="row">
                    <div class="row vertical-align">
  
                        <div class="col-md-10">
                            <div class="title">
                                <h1><center>FR-UGC: A Novel Full-reference Quality Metric for UGC Transcoding</center></h1>
                                <div class="container-sm">
                                    <div class="row justify-content-md-center">
									
									<div class="col-sm-auto">
                                            <p>Zihao Qi</p>
                                        </div>

									<div class="col-sm-auto">
                                            <p><a href='https://seis.bristol.ac.uk/~eexfz/index.htm'>Chen Feng</a></p>
                                        </div>

									<div class="col-sm-auto">
                                            <p><a href='https://danier97.github.io/'>Duolikun Danier</a></p>
                                        </div>

									<div class="col-sm-auto">
                                            <p><a href='https://seis.bristol.ac.uk/~eexfz/index.htm'>Fan Zhang</a></p>
                                        </div>
									
									<div class="col-sm-auto">
                                            <p>Shan Liu</p>
                                        </div>
									
									<div class="col-sm-auto">
                                            <p>Xiaozhong Xu</p>
                                        </div>
									
									<div class="col-sm-auto">
                                            <p><a href='https://david-bull.github.io/'>David Bull</a></p>
                                        </div>
									
                                    </div>
									<center><p>University of Bristol</p></center>
                                </div>
                            </div>
                        </div>
			<div class="col-md-2">
			    <p><a href='https://www.bristol.ac.uk'><img src="../uob-logo.svg" width="100%" height="20%" alt=""></a></p>
			    <p><a href='https://www.bristol.ac.uk/vision-institute'><img src="../bvilogo.png" width="100%" height="20%" alt=""></a></p>
			     <p><a href='https://vilab.blogs.bristol.ac.uk'><img src="../VILogo.jpg" width="100%" height="20%" alt=""></a></p>
			    
                        </div>
                    </div>
                </div>
            </div>
			
            <div class="container-md">
                <h2>Abstract</h2>
				<p>Unlike conventional video coding of pristine professional content, the delivery pipeline of User Generated Content (UGC) involves transcoding where unpristine reference videos need to be compressed repeatedly. In this work, we find that existing full-/no-reference quality metrics often assume the lack of coding-induced distortions in the reference sequences. Therefore these approaches cannot accurately predict the perceptual quality difference between  transcoded UGC content and the corresponding unpristine references, hence are unsuited for guiding the rate-distortion optimisation process in the transcoding systems. In this context, we propose a bespoke full-reference deep video quality metric for UGC transcoding. The proposed method features a transcoding-specific weakly supervised training strategy employing a quality ranking-based Siamese structure. The proposed method is evaluated on the YouTube-UGC VP9 subset and the LIVE-Wild database, showing state-of-the-art performance compared to existing VQA methods. </p>
            </div>
			<!--
            <hr />
            <div class="container-md">
                <h2 id="downloads">Useful Links [not yet available]</h2>
<p>[<a href="https://vilab.blogs.bristol.ac.uk/files/currently-not-available"><strong>DOWNLOAD</strong></a>] instructions and related file</p>
<p>[<a href="https://vilab.blogs.bristol.ac.uk/files/currently-not-available"><strong>DOWNLOAD</strong></a>] all videos</p>
<p>[<a href="https://vilab.blogs.bristol.ac.uk/files/currently-not-available"><strong>DOWNLOAD</strong></a>] all subjective data.</p>
Please read the README file before using the data.

            </div>-->
			
			<hr />
            <div class="container-md">
                <h2>Generation of FR-UGC training data.</h2>
				<p>Training datasets is generated in two steps. First pristine source sequences are compressed by H.264 to obtain distorted reference. Then, the distorted reference is further compressed by 4 popular codecs into 12 transcoded variants.</p>
				<p>To be noted, the sample frames are randomly picked. Every category contains both landscape videos and portrait videos (the ratio is roughly 1:1).</p>
                <center><img src="./generateTrain.svg" width="100%" alt=""> </center>
            </div>
			<!--
			<hr />
            <div class="container-md">
                <h2>Citation</h2>
				<pre class="citation">
@article{zihao2022bvi,
  title={BVI-UGC: Not yet available},
  author={Name1, Name2},
  journal={Journal},
  volume={111},
  number={222},
  pages={333},
  year={2022},
  publisher={IEEE}
}<A HREF="https://ieeexplore.ieee.org/abstract/document/not-yet-available">[paper]</a></pre>

            </div>
			<hr />
            

        -->
		</div>
    </div>
</body>

</html>
